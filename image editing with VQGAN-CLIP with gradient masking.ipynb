{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "another copy of Private of Active_Modified_VQGANCLIP_zquantize mask editor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8cc6064259b44398b596aa7f8a0ecc59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7aa4ab1c35e043e1a09e097642a5e545",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_031ac81470fc456fab994933e7148dc7",
              "IPY_MODEL_1187b51bea60402481fee0b32e31ad9b",
              "IPY_MODEL_2e31509fa71b4a43909ec8120aa76e4a"
            ]
          }
        },
        "7aa4ab1c35e043e1a09e097642a5e545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "031ac81470fc456fab994933e7148dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_19e0732eaaf545e683d4967e943c630a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17d4232014ec47fb84342801ae2a51ad"
          }
        },
        "1187b51bea60402481fee0b32e31ad9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_821f39e3fc8644bdbfed594a74180a24",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0fdc11074570408f8bb65fb1b1ed6bd6"
          }
        },
        "2e31509fa71b4a43909ec8120aa76e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_940bf8abfd954c1e8c801b618c0e02e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:06&lt;00:00, 92.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61573c72f63b4d5db4986decb5461a20"
          }
        },
        "19e0732eaaf545e683d4967e943c630a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17d4232014ec47fb84342801ae2a51ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "821f39e3fc8644bdbfed594a74180a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0fdc11074570408f8bb65fb1b1ed6bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "940bf8abfd954c1e8c801b618c0e02e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61573c72f63b4d5db4986decb5461a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ed94719ed7c40db99c0b494887d45c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_775479240c7647d08dfd89769330395f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bb87f3c773f54805a8551b5740714e4f",
              "IPY_MODEL_2f1918c8a30b4974a71ad3709d9379dd",
              "IPY_MODEL_132a16f370034bc39383ffae86bcf120"
            ]
          }
        },
        "775479240c7647d08dfd89769330395f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb87f3c773f54805a8551b5740714e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f87d2de1756643449f9f25c9596222f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a4cccc2184384afab4c43b626719ba6a"
          }
        },
        "2f1918c8a30b4974a71ad3709d9379dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_453ff1f6d46840579aae07e673546543",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76377bb26fdd4021b90044fcadec900b"
          }
        },
        "132a16f370034bc39383ffae86bcf120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cac8ae276d634733a1a1077a46458c1f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 129/? [17:17&lt;00:00,  8.03s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d4eac600fc5e4c5e8709a8c557ecb599"
          }
        },
        "f87d2de1756643449f9f25c9596222f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a4cccc2184384afab4c43b626719ba6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "453ff1f6d46840579aae07e673546543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76377bb26fdd4021b90044fcadec900b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cac8ae276d634733a1a1077a46458c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d4eac600fc5e4c5e8709a8c557ecb599": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generates images from text prompts with VQGAN and CLIP (z+quantize method) + editor masks.\n",
        "\n",
        "By https://twitter.com/jbusted1 .\n",
        "Based on a notebook by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n"
      ],
      "metadata": {
        "id": "CppIQlPhhwhs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@markdown Your GPU (p100 or v100 are preferable)\r\n",
        "!nvidia-smi"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkUfzT60ZZ9q",
        "cellView": "form",
        "outputId": "0ee9ebeb-395d-4ef6-9871-0ab3138dc2bf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import gc\r\n",
        "gc.collect()\r\n",
        "torch.cuda.empty_cache()"
      ],
      "outputs": [],
      "metadata": {
        "id": "rcNmhFFRBJkn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@markdown #install relevant libraries\r\n",
        "!git clone https://github.com/openai/CLIP\r\n",
        "!git clone https://github.com/CompVis/taming-transformers\r\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\r\n",
        "!pip install kornia\r\n",
        "!pip install einops"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wSfISAhyPmyp",
        "cellView": "form",
        "outputId": "57321953-d393-4678-ce0e-212273b2f9b5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@markdown #download vqgan weights\r\n",
        "\r\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_imagenet_f16_16384.yaml\r\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_imagenet_f16_16384.ckpt\r\n",
        "\r\n",
        "# !curl -L https://dl.nmkd.de/ai/clip/vqgan/8k-2021-06/vqgan-f8-8192.ckpt > vqgan_openimages_f16_8192.ckpt\r\n",
        "# !curl -L https://dl.nmkd.de/ai/clip/vqgan/8k-2021-06/vqgan-f8-8192.yaml > vqgan_openimages_f16_8192.yaml\r\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhhdWrSxQhwg",
        "cellView": "form",
        "outputId": "9e00cfcc-59fc-4f55-964e-a7b05ee53dcf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "#@markdown #import libraries\r\n",
        "import argparse\r\n",
        "import math\r\n",
        "from pathlib import Path\r\n",
        "import requests\r\n",
        "import sys\r\n",
        "import io\r\n",
        "\r\n",
        "sys.path.append('./taming-transformers')\r\n",
        "\r\n",
        "from IPython import display\r\n",
        "from omegaconf import OmegaConf\r\n",
        "from PIL import Image\r\n",
        "from taming.models import cond_transformer, vqgan\r\n",
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "from torch.nn import functional as F\r\n",
        "from torchvision import transforms\r\n",
        "from torchvision.transforms import functional as TF\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from CLIP import clip\r\n",
        "\r\n",
        "import kornia.augmentation as K"
      ],
      "outputs": [],
      "metadata": {
        "id": "EXMSuW2EQWsd",
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "#@markdown #helpful functions\r\n",
        "def noise_gen(shape):\r\n",
        "    n, c, h, w = shape\r\n",
        "    noise = torch.zeros([n, c, 1, 1])\r\n",
        "    for i in reversed(range(5)):\r\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\r\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\r\n",
        "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\r\n",
        "    return noise\r\n",
        "\r\n",
        "\r\n",
        "def sinc(x):\r\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\r\n",
        "\r\n",
        "\r\n",
        "def lanczos(x, a):\r\n",
        "    cond = torch.logical_and(-a < x, x < a)\r\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\r\n",
        "    return out / out.sum()\r\n",
        "\r\n",
        "\r\n",
        "def ramp(ratio, width):\r\n",
        "    n = math.ceil(width / ratio + 1)\r\n",
        "    out = torch.empty([n])\r\n",
        "    cur = 0\r\n",
        "    for i in range(out.shape[0]):\r\n",
        "        out[i] = cur\r\n",
        "        cur += ratio\r\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\r\n",
        "\r\n",
        "\r\n",
        "def resample(input, size, align_corners=True):\r\n",
        "    n, c, h, w = input.shape\r\n",
        "    dh, dw = size\r\n",
        "\r\n",
        "    input = input.view([n * c, 1, h, w])\r\n",
        "\r\n",
        "    if dh < h:\r\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\r\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\r\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\r\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\r\n",
        "\r\n",
        "    if dw < w:\r\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\r\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\r\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\r\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\r\n",
        "\r\n",
        "    input = input.view([n, c, h, w])\r\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\r\n",
        "    \r\n",
        "\r\n",
        "# def replace_grad(fake, real):\r\n",
        "#     return fake.detach() - real.detach() + real\r\n",
        "\r\n",
        "\r\n",
        "class ReplaceGrad(torch.autograd.Function):\r\n",
        "    @staticmethod\r\n",
        "    def forward(ctx, x_forward, x_backward):\r\n",
        "        ctx.shape = x_backward.shape\r\n",
        "        return x_forward\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def backward(ctx, grad_in):\r\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\r\n",
        "\r\n",
        "\r\n",
        "class ClampWithGrad(torch.autograd.Function):\r\n",
        "    @staticmethod\r\n",
        "    def forward(ctx, input, min, max):\r\n",
        "        ctx.min = min\r\n",
        "        ctx.max = max\r\n",
        "        ctx.save_for_backward(input)\r\n",
        "        return input.clamp(min, max)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def backward(ctx, grad_in):\r\n",
        "        input, = ctx.saved_tensors\r\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\r\n",
        "\r\n",
        "replace_grad = ReplaceGrad.apply\r\n",
        "\r\n",
        "clamp_with_grad = ClampWithGrad.apply\r\n",
        "# clamp_with_grad = torch.clamp\r\n",
        "\r\n",
        "def vector_quantize(x, codebook):\r\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\r\n",
        "    indices = d.argmin(-1)\r\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\r\n",
        "    return replace_grad(x_q, x)\r\n",
        "\r\n",
        "\r\n",
        "class Prompt(nn.Module):\r\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\r\n",
        "        super().__init__()\r\n",
        "        self.register_buffer('embed', embed)\r\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\r\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        \r\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\r\n",
        "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)\r\n",
        "\r\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\r\n",
        "        dists = dists * self.weight.sign()\r\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\r\n",
        "\r\n",
        "\r\n",
        "def parse_prompt(prompt):\r\n",
        "    vals = prompt.rsplit(':', 2)\r\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\r\n",
        "    return vals[0], float(vals[1]), float(vals[2])\r\n",
        "\r\n",
        "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\r\n",
        "    input_normed = F.normalize(input, dim=-1)\r\n",
        "    target_normed = F.normalize(target, dim=-1)\r\n",
        "    logits = input_normed @ target_normed.T * logit_scale\r\n",
        "    if labels is None:\r\n",
        "        labels = torch.arange(len(input), device=logits.device)\r\n",
        "    return F.cross_entropy(logits, labels)\r\n",
        "\r\n",
        "class MakeCutouts(nn.Module):\r\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\r\n",
        "        super().__init__()\r\n",
        "        self.cut_size = cut_size\r\n",
        "        self.cutn = cutn\r\n",
        "        self.cut_pow = cut_pow\r\n",
        "\r\n",
        "\r\n",
        "    def set_cut_pow(self, cut_pow):\r\n",
        "      self.cut_pow = cut_pow\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        sideY, sideX = input.shape[2:4]\r\n",
        "        max_size = min(sideX, sideY)\r\n",
        "        min_size = min(sideX, sideY, self.cut_size)\r\n",
        "        cutouts = []\r\n",
        "        cutouts_full = []\r\n",
        "        \r\n",
        "        min_size_width = min(sideX, sideY)\r\n",
        "        lower_bound = float(self.cut_size/min_size_width)\r\n",
        "        \r\n",
        "        for ii in range(self.cutn):\r\n",
        "            \r\n",
        "          if args.clip_model == 'ViT-B/16':\r\n",
        "            size = int(min_size_width*torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound, 1.))\r\n",
        "          else:\r\n",
        "            # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\r\n",
        "            size = int(min_size_width*torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound, 1.)) # replace .5 with a result for 224 the default large size is .95\r\n",
        "            ## size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\r\n",
        "\r\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\r\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\r\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\r\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\r\n",
        "\r\n",
        "        \r\n",
        "        cutouts = torch.cat(cutouts, dim=0)\r\n",
        "\r\n",
        "        # if args.use_augs:\r\n",
        "        #   cutouts = augs(cutouts)\r\n",
        "\r\n",
        "        # if args.noise_fac:\r\n",
        "        #   facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\r\n",
        "        #   cutouts = cutouts + facs * torch.randn_like(cutouts)\r\n",
        "        \r\n",
        "\r\n",
        "        return clamp_with_grad(cutouts, 0, 1)\r\n",
        "\r\n",
        "\r\n",
        "def load_vqgan_model(config_path, checkpoint_path):\r\n",
        "    config = OmegaConf.load(config_path)\r\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\r\n",
        "        model = vqgan.VQModel(**config.model.params)\r\n",
        "        model.eval().requires_grad_(False)\r\n",
        "        model.init_from_ckpt(checkpoint_path)\r\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\r\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\r\n",
        "        parent_model.eval().requires_grad_(False)\r\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\r\n",
        "        model = parent_model.first_stage_model\r\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\r\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\r\n",
        "        model.eval().requires_grad_(False)\r\n",
        "        model.init_from_ckpt(checkpoint_path)\r\n",
        "    else:\r\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\r\n",
        "    del model.loss\r\n",
        "    return model\r\n",
        "\r\n",
        "def resize_image(image, out_size):\r\n",
        "    ratio = image.size[0] / image.size[1]\r\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\r\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\r\n",
        "    return image.resize(size, Image.LANCZOS)\r\n",
        "\r\n",
        "def fetch(url_or_path):\r\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\r\n",
        "        r = requests.get(url_or_path)\r\n",
        "        r.raise_for_status()\r\n",
        "        fd = io.BytesIO()\r\n",
        "        fd.write(r.content)\r\n",
        "        fd.seek(0)\r\n",
        "        return fd\r\n",
        "    return open(url_or_path, 'rb')\r\n",
        "\r\n",
        "def visulize_regions(pil_image, z_mask):\r\n",
        "\r\n",
        "  OPACITY = int(255 * 0.5)\r\n",
        "\r\n",
        "  pil_image = pil_image.resize(size=(z_mask.shape[1]*16, z_mask.shape[0]*16))\r\n",
        "  overlay = Image.new('RGBA', pil_image.size, (0, 0, 0)+(0,))\r\n",
        "  d = ImageDraw.Draw(overlay)\r\n",
        "\r\n",
        "  ww,hh = z_mask.shape\r\n",
        "\r\n",
        "  for x in range(0,hh):\r\n",
        "    for y in range(0,ww):\r\n",
        "      if z_mask[y,x] == 0:\r\n",
        "        xx = x*16\r\n",
        "        yy = y*16\r\n",
        "        d.rectangle([(xx,yy),(xx+16,yy+16)],fill=(1,1,1)+(OPACITY,),width=16)\r\n",
        "\r\n",
        "  return Image.alpha_composite(pil_image, overlay)\r\n",
        "\r\n",
        "class TVLoss(nn.Module):\r\n",
        "    def forward(self, input):\r\n",
        "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\r\n",
        "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\r\n",
        "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\r\n",
        "        diff = x_diff**2 + y_diff**2 + 1e-8\r\n",
        "        return diff.mean(dim=1).sqrt().mean()\r\n",
        "\r\n",
        "class GaussianBlur2d(nn.Module):\r\n",
        "    def __init__(self, sigma, window=0, mode='reflect', value=0):\r\n",
        "        super().__init__()\r\n",
        "        self.mode = mode\r\n",
        "        self.value = value\r\n",
        "        if not window:\r\n",
        "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\r\n",
        "        if sigma:\r\n",
        "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\r\n",
        "            kernel /= kernel.sum()\r\n",
        "        else:\r\n",
        "            kernel = torch.ones([1])\r\n",
        "        self.register_buffer('kernel', kernel)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        n, c, h, w = input.shape\r\n",
        "        input = input.view([n * c, 1, h, w])\r\n",
        "        start_pad = (self.kernel.shape[0] - 1) // 2\r\n",
        "        end_pad = self.kernel.shape[0] // 2\r\n",
        "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\r\n",
        "        input = F.conv2d(input, self.kernel[None, None, None, :])\r\n",
        "        input = F.conv2d(input, self.kernel[None, None, :, None])\r\n",
        "        return input.view([n, c, h, w])\r\n",
        "\r\n",
        "class EMATensor(nn.Module):\r\n",
        "    \"\"\"implmeneted by Katherine Crowson\"\"\"\r\n",
        "    def __init__(self, tensor, decay):\r\n",
        "        super().__init__()\r\n",
        "        self.tensor = nn.Parameter(tensor)\r\n",
        "        self.register_buffer('biased', torch.zeros_like(tensor))\r\n",
        "        self.register_buffer('average', torch.zeros_like(tensor))\r\n",
        "        self.decay = decay\r\n",
        "        self.register_buffer('accum', torch.tensor(1.))\r\n",
        "        self.update()\r\n",
        "    \r\n",
        "    @torch.no_grad()\r\n",
        "    def update(self):\r\n",
        "        if not self.training:\r\n",
        "            raise RuntimeError('update() should only be called during training')\r\n",
        "\r\n",
        "        self.accum *= self.decay\r\n",
        "        self.biased.mul_(self.decay)\r\n",
        "        self.biased.add_((1 - self.decay) * self.tensor)\r\n",
        "        self.average.copy_(self.biased)\r\n",
        "        self.average.div_(1 - self.accum)\r\n",
        "\r\n",
        "    def forward(self):\r\n",
        "        if self.training:\r\n",
        "            return self.tensor\r\n",
        "        return self.average\r\n",
        "\r\n",
        "%mkdir /content/vids"
      ],
      "outputs": [],
      "metadata": {
        "id": "JvnTBhPGT1gn",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARGS VIT B32"
      ],
      "metadata": {
        "id": "2o7rru4587XA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "\r\n",
        "args = argparse.Namespace(\r\n",
        "    \r\n",
        "    prompts=[[\"a photo of the stary dark sky at night.\"],[\"a photo of a gothic palace at the street\"]], \r\n",
        "    size=[640, 512], \r\n",
        "    init_image= \"https://www.themontcalm.com/blog/wp-content/uploads/2018/04/piccadilly.jpg\", \r\n",
        "    init_weight=None,\r\n",
        "\r\n",
        "    clip_model='ViT-B/32',\r\n",
        "    vqgan_config='vqgan_imagenet_f16_16384.yaml',        \r\n",
        "    vqgan_checkpoint='vqgan_imagenet_f16_16384.ckpt', \r\n",
        "    step_size=0.25, \r\n",
        "    \r\n",
        "    cutn=50, \r\n",
        "    cut_pow=1, \r\n",
        "    \r\n",
        "    display_freq=25,\r\n",
        "    seed=1587585,\r\n",
        "    use_augs = True,\r\n",
        "    noise_fac= 0.1,\r\n",
        "    ema_val = 0.99,\r\n",
        "    \r\n",
        "    record_generation = True,\r\n",
        "    use_noise = None,\r\n",
        "    constraint_regions = False, \r\n",
        "    \r\n",
        "    noise_prompt_weights = None,\r\n",
        "    noise_prompt_seeds = [[14575]],\r\n",
        "   \r\n",
        "    decay_rate = 50,\r\n",
        "    _epoches = 5, \r\n",
        ")\r\n",
        "\r\n",
        "# ---\r\n",
        "\r\n",
        "# <AUGMENTATIONS>\r\n",
        "augs = nn.Sequential(\r\n",
        "    K.RandomHorizontalFlip(p=0.5),\r\n",
        "    K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\r\n",
        "    K.RandomPerspective(0.2,p=0.4, ),\r\n",
        "    K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\r\n",
        "    )\r\n",
        "\r\n",
        "noise = noise_gen([1, 3, args.size[0], args.size[1]])\r\n",
        "image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\r\n",
        "image.save('init3.png')"
      ],
      "outputs": [],
      "metadata": {
        "id": "tLw9p5Rzacso"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@markdown #Paint masks\r\n",
        "#@markdown the int input is the size of your brush\r\n",
        "from IPython.display import HTML\r\n",
        "from IPython.display import clear_output\r\n",
        "# from Ipython.display import Image as Image_\r\n",
        "from PIL import ImageDraw\r\n",
        "import IPython.core.display as display_\r\n",
        "from google.colab.output import eval_js\r\n",
        "from base64 import b64decode\r\n",
        "\r\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "canvas_html = \"\"\"\r\n",
        "<canvas height=%d width=%d style=\"%s\"></canvas>\r\n",
        "<button>Finish</button>\r\n",
        "<input type=\"number\" id=\"width\" name=\"width\" size=\"40\" value=\"50\">\r\n",
        "<input type=\"text\" id=\"color\" name=\"color\" size=\"40\">\r\n",
        "<script>\r\n",
        "var canvas = document.querySelector('canvas')\r\n",
        "var ctx = canvas.getContext('2d')\r\n",
        "ctx.lineCap = \"round\"\r\n",
        "ctx.lineJoin = \"round\"\r\n",
        "var button = document.querySelector('button')\r\n",
        "var mouse = {x: 0, y: 0}\r\n",
        "canvas.addEventListener('mousemove', function(e) {\r\n",
        "  mouse.x = e.pageX - this.offsetLeft\r\n",
        "  mouse.y = e.pageY - this.offsetTop\r\n",
        "})\r\n",
        "canvas.onmousedown = ()=>{\r\n",
        "  ctx.strokeStyle = document.getElementById(\"color\").value\r\n",
        "  ctx.lineWidth = document.getElementById(\"width\").value\r\n",
        "  ctx.beginPath()\r\n",
        "  ctx.moveTo(mouse.x, mouse.y)\r\n",
        "  canvas.addEventListener('mousemove', onPaint)\r\n",
        "}\r\n",
        "canvas.onmouseup = ()=>{\r\n",
        "  canvas.removeEventListener('mousemove', onPaint)\r\n",
        "}\r\n",
        "var onPaint = ()=>{\r\n",
        "  ctx.lineTo(mouse.x, mouse.y)\r\n",
        "  ctx.stroke()\r\n",
        "}\r\n",
        "var data = new Promise(resolve=>{\r\n",
        "  button.onclick = ()=>{\r\n",
        "    resolve(canvas.toDataURL('image/png'))\r\n",
        "  }\r\n",
        "})\r\n",
        "</script>\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "def draw(filename='drawing.png', w=640, h=512, image_path=\"\"):\r\n",
        "\r\n",
        "  f = 16\r\n",
        "  if(args.clip_model == 'ViT-B/16'):\r\n",
        "    f = 8\r\n",
        "\r\n",
        "  display_.display(HTML(canvas_html % (h, w, image_path)))\r\n",
        "  data = eval_js(\"data\")\r\n",
        "  binary = b64decode(data.split(',')[1])\r\n",
        "\r\n",
        "  clear_output()\r\n",
        "  \r\n",
        "  # pil_image = Image.open(filename).convert('RGBA')\r\n",
        "  pil_image = Image.open(io.BytesIO(binary)).convert('RGBA')\r\n",
        "  toksX, toksY = w // f, h // f\r\n",
        "  pil_image = TF.to_tensor(pil_image)[3].unsqueeze(0).unsqueeze(0)\r\n",
        "\r\n",
        "  return F.interpolate(pil_image, size=(int(args.size[1]/f), int(args.size[0]/f)), mode='nearest').squeeze()\r\n",
        "\r\n",
        "def get_z_mask():\r\n",
        "  image_path = \"\"\r\n",
        "\r\n",
        "  if args.init_image is not None:\r\n",
        "    image_path = f\"background: url('{args.init_image}')\"\r\n",
        "    pil_image = Image.open(fetch(args.init_image)).convert('RGBA')\r\n",
        "    w,h = pil_image.size\r\n",
        "    z_mask = draw(filename='drawing.png', w=w, h=h, image_path=image_path)\r\n",
        "  else:\r\n",
        "    \r\n",
        "    pil_image = None \r\n",
        "    image_path = \"border:1px solid #000000;\"\r\n",
        "    z_mask = draw(filename='drawing.png', image_path=image_path)\r\n",
        "  \r\n",
        "  return z_mask, pil_image\r\n",
        "\r\n",
        "z_masks = []\r\n",
        "for _ind in range(len(args.prompts)):\r\n",
        "  print(args.prompts[_ind])\r\n",
        "  z_mask, pil_image_ = get_z_mask()\r\n",
        "  z_masks.append(z_mask)\r\n",
        "  \r\n",
        "  \r\n",
        "\r\n",
        "for _ind in range(len(z_masks) - 1):\r\n",
        "  z_masks[_ind] = torch.max(z_masks[_ind] - sum(z_masks[_ind + 1:]), torch.zeros_like(z_masks[_ind]))\r\n",
        "\r\n",
        "\r\n",
        "for _ind in range(len(z_masks)):\r\n",
        "  if pil_image_ is None:\r\n",
        "    pil_image_ = Image.new(\"RGBA\", (800, 1280), (255, 255, 255))\r\n",
        "  print(args.prompts[_ind])\r\n",
        "  h = visulize_regions(pil_image_, z_masks[_ind])\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "  h.save(f'maskvis_{_ind}.png')\r\n",
        "  display.display(display.Image(f'maskvis_{_ind}.png'))\r\n",
        "\r\n",
        "\r\n",
        "  z_masks[_ind] = z_masks[_ind].unsqueeze(0).unsqueeze(0).repeat(1,256,1,1).to(device)\r\n",
        "  \r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "# https://i.imgur.com/7k0YQWK.png"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "afJdrUP2lvPW",
        "cellView": "form",
        "outputId": "245b9088-4382-4a55-ae27-f55330149e63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actually do the run..."
      ],
      "metadata": {
        "id": "QXgTa_JWi7Sn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "print('Using device:', device)\r\n",
        "print('using prompts: ', args.prompts)\r\n",
        "\r\n",
        "tv_loss = TVLoss() \r\n",
        "\r\n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\r\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\r\n",
        "mse_weight = args.init_weight\r\n",
        "\r\n",
        "cut_size = perceptor.visual.input_resolution\r\n",
        "# e_dim = model.quantize.e_dim\r\n",
        "\r\n",
        "if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\r\n",
        "    e_dim = 256\r\n",
        "    n_toks = model.quantize.n_embed\r\n",
        "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\r\n",
        "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\r\n",
        "else:\r\n",
        "    e_dim = model.quantize.e_dim\r\n",
        "    n_toks = model.quantize.n_e\r\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\r\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\r\n",
        "\r\n",
        "\r\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\r\n",
        "\r\n",
        "f = 2**(model.decoder.num_resolutions - 1)\r\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\r\n",
        "\r\n",
        "if args.seed is not None:\r\n",
        "    torch.manual_seed(args.seed)\r\n",
        "\r\n",
        "if args.init_image:\r\n",
        "    pil_image = Image.open(fetch(args.init_image)).convert('RGB')\r\n",
        "    pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\r\n",
        "    pil_image = TF.to_tensor(pil_image)\r\n",
        "    if args.use_noise:\r\n",
        "      pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \r\n",
        "    z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\r\n",
        "\r\n",
        "else:\r\n",
        "    \r\n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\r\n",
        "\r\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\r\n",
        "        z = one_hot @ model.quantize.embed.weight\r\n",
        "    else:\r\n",
        "        z = one_hot @ model.quantize.embedding.weight\r\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\r\n",
        "\r\n",
        "\r\n",
        "z = EMATensor(z, args.ema_val)\r\n",
        "\r\n",
        "z_orig = z.tensor.clone()\r\n",
        "\r\n",
        "\r\n",
        "opt = optim.Adam(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\r\n",
        "\r\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\r\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\r\n",
        "pMss = []\r\n",
        "for p_index in range(len(args.prompts)):\r\n",
        "  pMs = []\r\n",
        "\r\n",
        "  if args.noise_prompt_weights and args.noise_prompt_seeds:\r\n",
        "    for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\r\n",
        "      gen = torch.Generator().manual_seed(seed)\r\n",
        "      embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\r\n",
        "      pMs.append(Prompt(embed, weight).to(device))\r\n",
        "\r\n",
        "  for prompt in args.prompts[p_index]:\r\n",
        "      txt, weight, stop = parse_prompt(prompt)\r\n",
        "      embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\r\n",
        "      pMs.append(Prompt(embed, weight, stop).to(device))\r\n",
        "      # pMs[0].embed = pMs[0].embed + Prompt(embed, weight, stop).embed.to(device)\r\n",
        "\r\n",
        "  pMss.append(pMs)\r\n",
        "\r\n",
        "def synth(z, quantize=True):\r\n",
        "    # if args.constraint_regions:\r\n",
        "    #   z = replace_grad(z, z * z_mask)\r\n",
        "\r\n",
        "    if quantize:\r\n",
        "      if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\r\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\r\n",
        "      else:\r\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\r\n",
        "\r\n",
        "    else:\r\n",
        "      z_q = z.model\r\n",
        "\r\n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\r\n",
        "\r\n",
        "@torch.no_grad()\r\n",
        "def checkin(i, losses):\r\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\r\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\r\n",
        "\r\n",
        "    print(\"averaged:\")\r\n",
        "    out = synth(z.average, True)\r\n",
        "    TF.to_pil_image(out[0].cpu()).save('progress.png')   \r\n",
        "    display.display(display.Image('progress.png')) \r\n",
        "\r\n",
        "def get_encodings():\r\n",
        "    \r\n",
        "    out = synth(z.tensor)\r\n",
        "\r\n",
        "    if args.record_generation:\r\n",
        "      with torch.no_grad():\r\n",
        "        global vid_index\r\n",
        "        out_a = synth(z.average, True)\r\n",
        "        TF.to_pil_image(out_a[0].cpu()).save(f'/content/vids/{vid_index}.png')\r\n",
        "        vid_index += 1\r\n",
        "    \r\n",
        "    cutouts = make_cutouts(out)\r\n",
        "\r\n",
        "    if args.use_augs:\r\n",
        "      cutouts = augs(cutouts)\r\n",
        "\r\n",
        "    if args.noise_fac:\r\n",
        "      facs = cutouts.new_empty([args.cutn, 1, 1, 1]).uniform_(0, args.noise_fac)\r\n",
        "      cutouts = cutouts + facs * torch.randn_like(cutouts)\r\n",
        "\r\n",
        "    return perceptor.encode_image(normalize(cutouts)).float()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def ascend_txt(iii, m_i, pMss):\r\n",
        "    global z_orig   \r\n",
        "\r\n",
        "    result = []\r\n",
        "    if args.init_weight:\r\n",
        "      result.append(F.mse_loss(z.tensor, z_orig) * mse_weight / 2)        \r\n",
        "\r\n",
        "    for prompt in pMss:\r\n",
        "        result.append(prompt(iii))\r\n",
        "\r\n",
        "    return result\r\n",
        "\r\n",
        "vid_index = 0\r\n",
        "def train(i):\r\n",
        "    \r\n",
        "    opt.zero_grad()\r\n",
        "\r\n",
        "    \r\n",
        "    iii = get_encodings()\r\n",
        "    for m_i in range(len(args.prompts)):\r\n",
        "      \r\n",
        "      h = z.tensor.register_hook(lambda grad: grad * z_masks[m_i])\r\n",
        "\r\n",
        "      lossAll = ascend_txt(iii, m_i, pMss[m_i])\r\n",
        "      loss = sum(lossAll)\r\n",
        "      loss.backward(retain_graph=True)\r\n",
        "\r\n",
        "      h.remove()\r\n",
        "\r\n",
        "    opt.step()\r\n",
        "    z.update()\r\n",
        "\r\n",
        "    if i % args.display_freq == 0:\r\n",
        "          checkin(i, lossAll)\r\n",
        "\r\n",
        "    # if args.init_weight and args.use_mse_sch:\r\n",
        "    #   update_mse()\r\n",
        "\r\n",
        "i = 0\r\n",
        "try:\r\n",
        "    \r\n",
        "    with tqdm() as pbar:\r\n",
        "        while True and i < 3000:           \r\n",
        "\r\n",
        "            train(i)\r\n",
        "\r\n",
        "            if i > 0 and i%args.decay_rate==0 and i <= args.decay_rate * args._epoches:\r\n",
        "              z = EMATensor(z.average, args.ema_val)\r\n",
        "              opt = optim.Adam(z.parameters(), lr=args.step_size, weight_decay=0.00000000) \r\n",
        "              print(\"restarting optimization\")\r\n",
        "            i += 1\r\n",
        "            pbar.update()\r\n",
        "\r\n",
        "except KeyboardInterrupt:\r\n",
        "    pass\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "g7EDme5RYCrt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8cc6064259b44398b596aa7f8a0ecc59",
            "7aa4ab1c35e043e1a09e097642a5e545",
            "031ac81470fc456fab994933e7148dc7",
            "1187b51bea60402481fee0b32e31ad9b",
            "2e31509fa71b4a43909ec8120aa76e4a",
            "19e0732eaaf545e683d4967e943c630a",
            "17d4232014ec47fb84342801ae2a51ad",
            "821f39e3fc8644bdbfed594a74180a24",
            "0fdc11074570408f8bb65fb1b1ed6bd6",
            "940bf8abfd954c1e8c801b618c0e02e7",
            "61573c72f63b4d5db4986decb5461a20",
            "3ed94719ed7c40db99c0b494887d45c0",
            "775479240c7647d08dfd89769330395f",
            "bb87f3c773f54805a8551b5740714e4f",
            "2f1918c8a30b4974a71ad3709d9379dd",
            "132a16f370034bc39383ffae86bcf120",
            "f87d2de1756643449f9f25c9596222f3",
            "a4cccc2184384afab4c43b626719ba6a",
            "453ff1f6d46840579aae07e673546543",
            "76377bb26fdd4021b90044fcadec900b",
            "cac8ae276d634733a1a1077a46458c1f",
            "d4eac600fc5e4c5e8709a8c557ecb599"
          ]
        },
        "outputId": "faf73a73-c272-402f-b1d9-4d1f54d00d15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create video"
      ],
      "metadata": {
        "id": "CDUaCaRnUKMZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%cd vids\r\n",
        "\r\n",
        "images = \"%d.png\"\r\n",
        "video = \"/content/monster2.mp4\"\r\n",
        "!ffmpeg -r 60 -i $images -crf 20 -s 640x512 -pix_fmt yuv420p $video\r\n",
        "\r\n",
        "%cd .."
      ],
      "outputs": [],
      "metadata": {
        "id": "DT3hKb5gJUPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdeb7a91-c2ac-4725-ef3f-31c2bb379153"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "delete all frames from folder"
      ],
      "metadata": {
        "id": "UiZMW3kAUD1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%cd vids\r\n",
        "%rm *.png\r\n",
        "%cd .."
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsixT6gqJ8aY",
        "outputId": "da3b2990-9ad7-45c9-b50f-44447cd8fe9b"
      }
    }
  ]
}